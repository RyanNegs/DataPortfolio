{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1b7dfd1-b0cc-45df-a3cd-0c27323993c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6f85840-81a3-4493-b96c-f51bc7073177",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8fc8886-e13d-41f3-9cba-9ac64bc2d345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/jenn/Library/CloudStorage/OneDrive-UniversityofPittsburgh/Shared Documents - Assessment and QA/General/Field Experience-24/Jenn-tempFiles'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "#os.chdir('C:\\\\Users\\\\jym20\\\\OneDrive - University of Pittsburgh\\Shared Documents - Assessment and QA\\\\General\\\\Field Experience-24\\\\Jenn-tempFiles')\n",
    "os.chdir('/Users/jenn/Library/CloudStorage/OneDrive-UniversityofPittsburgh/Shared Documents - Assessment and QA/General/Field Experience-24/Jenn-tempFiles')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "481243c8-bf17-412f-bb1b-bce2b3812ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_df = pd.read_csv('deidentified_transcript.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1dcf7ce-1638-4fca-bfc6-6adc0e51d666",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from pandas import DataFrame\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from fuzzywuzzy import process\n",
    "import nltk\n",
    "import logging\n",
    "from hashlib import blake2b\n",
    "import re\n",
    "from fuzzywuzzy import fuzz\n",
    "import hashlib\n",
    "import html\n",
    "import emoji\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ccb3133-7519-4fc4-9bf7-95b21ad734c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10546 entries, 0 to 10545\n",
      "Data columns (total 23 columns):\n",
      " #   Column                         Non-Null Count  Dtype  \n",
      "---  ------                         --------------  -----  \n",
      " 0   Chat ID                        10546 non-null  int64  \n",
      " 1   Browser                        10546 non-null  object \n",
      " 2   Operating System               10541 non-null  object \n",
      " 3   User Agent                     10546 non-null  object \n",
      " 4   Referrer                       10546 non-null  object \n",
      " 5   Widget                         10544 non-null  object \n",
      " 6   Department                     10382 non-null  object \n",
      " 7   Timestamp                      10546 non-null  object \n",
      " 8   Wait Time (seconds)            10546 non-null  int64  \n",
      " 9   Duration (seconds)             10546 non-null  int64  \n",
      " 10  Screensharing                  32 non-null     object \n",
      " 11  Rating (0-4)                   10546 non-null  int64  \n",
      " 12  Comment                        562 non-null    object \n",
      " 13  Transfer History               245 non-null    object \n",
      " 14  Message Count                  10546 non-null  int64  \n",
      " 15  Internal Note                  49 non-null     object \n",
      " 16  Tags                           2 non-null      object \n",
      " 17  Ticket ID                      2111 non-null   float64\n",
      " 18  Unnamed: 27                    116 non-null    object \n",
      " 19  Deidentified_Answerer          10546 non-null  object \n",
      " 20  Deidentified_Email             10546 non-null  object \n",
      " 21  Deidentified_Transcript        10546 non-null  object \n",
      " 22  Deidentified_Initial_Question  9577 non-null   object \n",
      "dtypes: float64(1), int64(5), object(17)\n",
      "memory usage: 1.9+ MB\n"
     ]
    }
   ],
   "source": [
    "chat_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1b067a5-a0b7-4189-aa0a-46ecda681dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_df['Deidentified_Transcript'].fillna('Na', inplace=True)\n",
    "chat_df['Deidentified_Initial_Question'].fillna('Na', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8dd26fd2-75e7-4bea-8350-0cf094b2720f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_df=chat_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c27685d-1513-4082-8364-29c00bb524e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_df['full_chat'] = chat_df['Deidentified_Initial_Question'] + ' ' + chat_df['Deidentified_Transcript']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e895b564-68bc-4483-a064-66f6407fe5a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi! I am a faculty member and had a book on reserve for last term. I want the same book on reserve this term. It should still be in the library. Do I need to fill out another form? 11:00:11 - Librarian 6 : Hello, thank for chatting. I believe you ULSMember still have to complete another form, but let me double check with out course reserve staff. I will just be a minute.\n",
      "11:02:53 - Librarian 6 : So, you will need to fill out another form. Which book is it? Is it a personal copy or part of the hillman collection? I can check it if it still in our office/on the reserve shelf.\n",
      "11:06:05 - Userd881f9a61c : thanks!\n",
      "11:06:15 - Userd881f9a61c : Ittrade_mark s Motivation Science\n",
      "11:06:32 - Userd881f9a61c : Ittrade_mark s a personal copy for course Psy 9999\n",
      "11:08:33 - Userd881f9a61c : thanks for your help!\n",
      "11:08:45 - Librarian 6 : okay, it is still here. you're welcome!\n",
      "11:09:43 - Userd881f9a61c : awesome, so i will fill out another form and drop it by\n",
      "11:09:45 - Userd881f9a61c : thanks!\n",
      "11:09:59 - Librarian 6 : sure, have ULSMember one\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(chat_df['full_chat'].iloc[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733492bf-4f17-4197-901f-58ff709acd6f",
   "metadata": {},
   "source": [
    "## 1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d715530-a55c-48c6-8892-d0d2d6db9e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6528fc0-533e-404e-9ca6-3f3e99536bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Remove timestamps, identifiers, URLs, and non-ASCII characters\n",
    "    text = re.sub(r'\\d{2}:\\d{2}:\\d{2}', '', text)\n",
    "    text = re.sub(r'- Librarian \\d+ :', '', text)\n",
    "    text = re.sub(r'User[a-zA-Z0-9]+ :', '', text)\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "027201ef-37e9-428c-83d4-7c6d995e2211",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_df['Deidentified_Transcript_clean'] = chat_df['Deidentified_Transcript'].apply(preprocess_text)\n",
    "chat_df['Deidentified_Initial_Question_clean'] = chat_df['Deidentified_Initial_Question'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d18ad0a8-ed13-4542-8945-e43d5eec40a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appply Preprocess_text function\n",
    "chat_df['full_chat_clean'] = chat_df['full_chat'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aec40634-78ea-43b3-b6fc-76e8ec02fc1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Trying to help a faculty member who's in the process of submitting an article for publication with a former graduate student (now alumni). She's asking if there is a way to get that student access to journal articles so that they can finish their response to reviewers. I know the special borrowers card would get her access to print materials, but is there any alternative mechanism for online databases?   [nameredacted] [nameredacted],\\n  at Hillman library the alumni can get a day pass to access our databases in the library\\n  I don't think there is a mechanism that would allow the student to have remote access\\n  maybe a sponsored CSSD account through your department, but I'm not sure if that would provide access to library databases.\\n -  Hmm, OK. If they paid for a special borrowertrade_mark s card, would that have any other benefit that would help them with remotely requesting materials?\\n  the special borrowers card only gives the ability to check out materials from the print collection, so unfortunately no that wouldn't help\\n -  OK. Thanks. Just want to make sure I give them the right information.\\n  sure thing, please ask again if you have any questions.\\n -  Bye.\\n\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_df['full_chat_clean'].iloc[6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fdaa5e-9322-4ca0-a1d9-6d9415897ffe",
   "metadata": {},
   "source": [
    "## TEST 1. NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ccb6d8f5-1fdc-4a1b-aaeb-59f9fba06fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/jenn/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/jenn/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/jenn/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to /Users/jenn/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Initialize lemmatizer and stopwords\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfadbd8c-ab6a-4e35-a961-38a52d18bab9",
   "metadata": {},
   "source": [
    "### Lemmatizes and Filters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3320ab8b-c383-4fa5-aec0-196047f2bc65",
   "metadata": {},
   "source": [
    "The text based on specified parts of speech (POS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e75b841-a605-4738-a9ac-668e23c67676",
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_stopwords = set([\n",
    "    \"nameredacted\", \"librarian\", \"hi\", \"hello\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b6a08326-3b33-4edf-92f8-92fa16e5164a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')).union(additional_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2aa0925f-a468-4802-b999-8f429f9c5481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to lemmatize and filter POS\n",
    "def lemmatize_and_filter(text, pos_tags):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    words = [word for word in words if word.isalnum() and word.lower() not in stop_words]\n",
    "    pos_words = nltk.pos_tag(words)\n",
    "    filtered_words = [lemmatizer.lemmatize(word.lower()) for word, pos in pos_words if pos in pos_tags]\n",
    "    return ' '.join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bce14b7c-55e7-4b37-8aa8-c90a70f944e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the different datasets\n",
    "question_only_df = chat_df[['Deidentified_Initial_Question_clean']].copy()\n",
    "question_only_df['processed'] = question_only_df['Deidentified_Initial_Question_clean'].apply(lambda x: lemmatize_and_filter(x, {'NN', 'JJ', 'VB'}))\n",
    "\n",
    "whole_chat_df = chat_df[['full_chat_clean']].copy()\n",
    "whole_chat_df['processed'] = whole_chat_df['full_chat_clean'].apply(lambda x: lemmatize_and_filter(x, {'NN', 'JJ', 'VB'}))\n",
    "\n",
    "whole_chat_nouns_adjectives_df = chat_df[['full_chat_clean']].copy()\n",
    "whole_chat_nouns_adjectives_df['processed'] = whole_chat_nouns_adjectives_df['full_chat_clean'].apply(lambda x: lemmatize_and_filter(x, {'NN', 'JJ'}))\n",
    "\n",
    "whole_chat_nouns_adjectives_verbs_df = chat_df[['full_chat_clean']].copy()\n",
    "whole_chat_nouns_adjectives_verbs_df['processed'] = whole_chat_nouns_adjectives_verbs_df['full_chat_clean'].apply(lambda x: lemmatize_and_filter(x, {'NN', 'JJ', 'VB'}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "95781a5e-9a6f-4b27-b7b1-7e91ce013366",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A CountVectorizer is initialized to convert the text data into a document-term matrix (DTM). The parameters used are:\\n\\t•\\tmax_df=0.95: Ignore terms that appear in more than 80% of the documents.\\n\\t•\\tmin_df=2: Ignore terms that appear in fewer than 5 documents.\\n\\t•\\tstop_words='english': Remove common English stop words.\\n\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to perform LDA and format topics\n",
    "def perform_lda(text_data, num_topics=15):\n",
    "    vectorizer = CountVectorizer(max_df=0.80, min_df=5, stop_words='english')\n",
    "    dtm = vectorizer.fit_transform(text_data)\n",
    "    lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "    lda.fit(dtm)\n",
    "    topics = []\n",
    "    for idx, topic in enumerate(lda.components_):\n",
    "        topic_words = [vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-10:]]\n",
    "        topics.append(f\"Topic {idx + 1}.\" + \" \".join(topic_words))\n",
    "    return topics\n",
    "\n",
    "\"\"\"A CountVectorizer is initialized to convert the text data into a document-term matrix (DTM). The parameters used are:\n",
    "\t•\tmax_df=0.95: Ignore terms that appear in more than 80% of the documents.\n",
    "\t•\tmin_df=2: Ignore terms that appear in fewer than 5 documents.\n",
    "\t•\tstop_words='english': Remove common English stop words.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9974b9e6-3b32-43b4-a4b0-4fd06116e89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LDA on the different datasets\n",
    "question_only_topics = perform_lda(question_only_df['processed'])\n",
    "whole_chat_topics = perform_lda(whole_chat_df['processed'])\n",
    "whole_chat_nouns_adjectives_topics = perform_lda(whole_chat_nouns_adjectives_df['processed'])\n",
    "whole_chat_nouns_adjectives_verbs_topics = perform_lda(whole_chat_nouns_adjectives_verbs_df['processed'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1cc3065f-548d-42e4-bffe-1aa3d9558206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Topic 1.program health possible access summer like graduate research library student',\n",
       " 'Topic 2.available process chapter like scan course item copy book request',\n",
       " 'Topic 3.service year print number laptop card new renew library borrow',\n",
       " 'Topic 4.list academic use dissertation special citation style database page information',\n",
       " 'Topic 5.need issue copy loan request trouble volume help journal article',\n",
       " 'Topic 6.rent week able available ebook today time library check open',\n",
       " 'Topic 7.text database able link subscription article library way online access',\n",
       " 'Topic 8.pdf access computer ask research article use able help download',\n",
       " 'Topic 9.course copy version film know let question library online available',\n",
       " 'Topic 10.reference address review video thesis class research paper need help',\n",
       " 'Topic 11.chapter date renew today check overdue possible return library book',\n",
       " 'Topic 12.issue page try log error login message access account library',\n",
       " 'Topic 13.artist license entry flow scanner payment master team locate na',\n",
       " 'Topic 14.author group title way old page floor study room search',\n",
       " 'Topic 15.way today available staff hold member faculty library pick book']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_only_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "682546c1-039a-48c1-a1a2-787f17710e28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Topic 1.address thank create day great contact help question ticket email',\n",
       " 'Topic 2.help title loan check item library copy available request book',\n",
       " 'Topic 3.email today number renew check help card able library account',\n",
       " 'Topic 4.moment reserve information thank available check form link course page',\n",
       " 'Topic 5.student study library welcome today sure great thank day help',\n",
       " 'Topic 6.document sure thesis style submit citation review dissertation school help',\n",
       " 'Topic 7.available moment look able download help book link online access',\n",
       " 'Topic 8.check help desk ground class open return floor library book',\n",
       " 'Topic 9.use information great faculty help day student access library br',\n",
       " 'Topic 10.delay additional assist inquiry response help possible time feel free',\n",
       " 'Topic 11.page sure issue help error message let email problem try',\n",
       " 'Topic 12.try let title page help link request journal access article',\n",
       " 'Topic 13.topic try subject use specific information database help research search',\n",
       " 'Topic 14.word list figure file template help br table page document',\n",
       " 'Topic 15.need look moment thank able help paper library database access']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_chat_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "655d743b-37b0-481c-a94e-56fa08bb3c84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Topic 1.question day link ok ask great record additional help br',\n",
       " 'Topic 2.faculty available free email help online question able library access',\n",
       " 'Topic 3.faculty great staff contact available student day service loan library',\n",
       " 'Topic 4.section caption help text document figure list style page table',\n",
       " 'Topic 5.specific ok box research type help use library database search',\n",
       " 'Topic 6.number contact create sure day great email question help ticket',\n",
       " 'Topic 7.help drop day study ground room thank library open floor',\n",
       " 'Topic 8.check chapter moment title edition library request copy available book',\n",
       " 'Topic 9.place storage link scan option available page library item request',\n",
       " 'Topic 10.day specific thank moment contact great research question help information',\n",
       " 'Topic 11.online download issue problem click article able page link access',\n",
       " 'Topic 12.help moment date renew today day able card account library',\n",
       " 'Topic 13.share review version dissertation sure template file school document help',\n",
       " 'Topic 14.loan link citation thank help moment request access journal article',\n",
       " 'Topic 15.moment today ticket day contact help great address thank email']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_chat_nouns_adjectives_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8ef2617a-bc59-474c-9bfc-7c8620305a13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Topic 1.address thank create day great contact help question ticket email',\n",
       " 'Topic 2.help title loan check item library copy available request book',\n",
       " 'Topic 3.email today number renew check help card able library account',\n",
       " 'Topic 4.moment reserve information thank available check form link course page',\n",
       " 'Topic 5.student study library welcome today sure great thank day help',\n",
       " 'Topic 6.document sure thesis style submit citation review dissertation school help',\n",
       " 'Topic 7.available moment look able download help book link online access',\n",
       " 'Topic 8.check help desk ground class open return floor library book',\n",
       " 'Topic 9.use information great faculty help day student access library br',\n",
       " 'Topic 10.delay additional assist inquiry response help possible time feel free',\n",
       " 'Topic 11.page sure issue help error message let email problem try',\n",
       " 'Topic 12.try let title page help link request journal access article',\n",
       " 'Topic 13.topic try subject use specific information database help research search',\n",
       " 'Topic 14.word list figure file template help br table page document',\n",
       " 'Topic 15.need look moment thank able help paper library database access']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_chat_nouns_adjectives_verbs_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0467841-a0a5-4a6c-a08d-910019cd24cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d1deb1-3877-40b8-846e-529d028efb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refined and only see \"whole_chat_nouns_adjectives_verbs_topics\"\n",
    "\n",
    "# import pandas as pd\n",
    "# import re\n",
    "# import nltk\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "# from nltk.corpus import stopwords\n",
    "# from sklearn.decomposition import LatentDirichletAllocation\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# # Assuming chat_df is already defined with your data\n",
    "# # Initialize lemmatizer and stopwords\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "additional_stopwords = set([\n",
    "    \"nameredacted\", \"librarian\", \"hi\", \"hello\", \"ULS\"\n",
    "])\n",
    "\n",
    "stop_words = stop_words.union(additional_stopwords)\n",
    "\n",
    "# Function to lemmatize and filter POS\n",
    "def lemmatize_and_filter(text, pos_tags):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    words = [word for word in words if word.isalnum() and word.lower() not in stop_words]\n",
    "    pos_words = nltk.pos_tag(words)\n",
    "    filtered_words = [lemmatizer.lemmatize(word.lower()) for word, pos in pos_words if pos in pos_tags]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Create the different datasets\n",
    "question_only_df = chat_df[['Deidentified_Initial_Question_clean']].copy()\n",
    "question_only_df['processed'] = question_only_df['Deidentified_Initial_Question_clean'].apply(lambda x: lemmatize_and_filter(x, {'NN', 'JJ', 'VB'}))\n",
    "\n",
    "whole_chat_df = chat_df[['full_chat_clean']].copy()\n",
    "whole_chat_df['processed'] = whole_chat_df['full_chat_clean'].apply(lambda x: lemmatize_and_filter(x, {'NN', 'JJ', 'VB'}))\n",
    "\n",
    "whole_chat_nouns_adjectives_df = chat_df[['full_chat_clean']].copy()\n",
    "whole_chat_nouns_adjectives_df['processed'] = whole_chat_nouns_adjectives_df['full_chat_clean'].apply(lambda x: lemmatize_and_filter(x, {'NN', 'JJ'}))\n",
    "\n",
    "whole_chat_nouns_adjectives_verbs_df = chat_df[['full_chat_clean']].copy()\n",
    "whole_chat_nouns_adjectives_verbs_df['processed'] = whole_chat_nouns_adjectives_verbs_df['full_chat_clean'].apply(lambda x: lemmatize_and_filter(x, {'NN', 'JJ', 'VB'}))\n",
    "\n",
    "# Function to perform LDA and format topics\n",
    "def perform_lda(text_data, num_topics=15):\n",
    "    vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "    dtm = vectorizer.fit_transform(text_data)\n",
    "    lda = LatentDirichletAllocation(n_components=num_topics, random_state=7)\n",
    "    lda.fit(dtm)\n",
    "    doc_topic_matrix = lda.transform(dtm)\n",
    "    topics = []\n",
    "    for idx, topic in enumerate(lda.components_):\n",
    "        topic_words = [vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-10:]]\n",
    "        topics.append(f\"Topic {idx + 1}.\" + \" \".join(topic_words))\n",
    "    return lda, doc_topic_matrix, topics\n",
    "\n",
    "# Apply LDA on the 'processed' column of the whole_chat_nouns_adjectives_verbs_df\n",
    "num_topics = 15\n",
    "lda_model, doc_topic_matrix, topics = perform_lda(whole_chat_nouns_adjectives_verbs_df['processed'], num_topics)\n",
    "\n",
    "# Assign each document to the most probable topic\n",
    "chat_df['lda_topic'] = doc_topic_matrix.argmax(axis=1)\n",
    "\n",
    "# Display the topics\n",
    "print(\"LDA Topics:\")\n",
    "for topic in topics:\n",
    "    print(topic)\n",
    "\n",
    "# Display the DataFrame with LDA topics assigned\n",
    "print(\"Chat DataFrame with LDA Topics:\")\n",
    "print(chat_df[['full_chat_clean', 'lda_topic']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddf533f-c110-4b03-b51b-49f2a0ff1229",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_df['lda_topic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d072ffc-0f9c-4936-813c-c80cb0ab6fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_df[chat_df['lda_topic']==12].full_chat_clean.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13470368-3c1a-41ce-ab47-3f1508b6f514",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "742612f7-a022-462a-ada5-914bbf159298",
   "metadata": {},
   "source": [
    "### First Half Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "288c1799-9662-4e52-a16b-3b3012a65eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/jenn/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/jenn/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/jenn/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to /Users/jenn/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA Topics:\n",
      "Topic 1.title edition chapter moment copy available library check request book\n",
      "Topic 2.volume digital help available need paper library request storage copy\n",
      "Topic 3.email check help today look thank moment item available request\n",
      "Topic 4.need record catalog available check library request interlibrary access loan\n",
      "Topic 5.date check floor number today return renew account book library\n",
      "Topic 6.equipment specialist topic course database project subject search research class\n",
      "Topic 7.able moment address help contact question account ticket library email\n",
      "Topic 8.room study staff school member faculty access library help student\n",
      "Topic 9.able page search moment look online link journal article access\n",
      "Topic 10.need question research help permission copyright public open use library\n",
      "Topic 11.guide page style dissertation moment help question search citation information\n",
      "Topic 12.list share figure sure file template table page document help\n",
      "Topic 13.check review time document link sure ok school help br\n",
      "Chat DataFrame with LDA Topics:\n",
      "                                         full_chat_clean lda_topic\n",
      "0      I would like to recommend adding to our Pitt r...         5\n",
      "1      sorry our chat disappeared   Hi\\n  : Well here...         0\n",
      "2      I would like to borrow a book from the law lib...        10\n",
      "3      hurricane disaster management   hi\\n  what are...         0\n",
      "4      yes.. looking for an article that comes up at ...         8\n",
      "...                                                  ...       ...\n",
      "10541  Hello! I'm a staff member at Pitt, and I wante...         7\n",
      "10542  I want inquire on the due date of a book I bor...         2\n",
      "10543  I get \"There seems to be an issue with your li...         6\n",
      "10544  what is an ISSN   An ISSN is a number that ide...         8\n",
      "10545  Hi! I was hoping to see if Pitt has access to ...         8\n",
      "\n",
      "[10546 rows x 2 columns]\n",
      "Topic Counts:\n",
      "lda_topic\n",
      "8     2311\n",
      "0     1655\n",
      "6     1106\n",
      "4      876\n",
      "11     801\n",
      "7      753\n",
      "2      675\n",
      "10     615\n",
      "3      472\n",
      "1      464\n",
      "9      367\n",
      "5      364\n",
      "12      87\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Assuming chat_df is already defined with your data\n",
    "# Initialize lemmatizer and stopwords\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "additional_stopwords = set([\n",
    "    \"nameredacted\", \"librarian\", \"hi\", \"hello\"\n",
    "])\n",
    "\n",
    "stop_words = stop_words.union(additional_stopwords)\n",
    "\n",
    "\n",
    "# Function to get the first half of the conversation\n",
    "def get_first_half(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    first_half = sentences[:len(sentences) // 2]\n",
    "    return ' '.join(first_half)\n",
    "\n",
    "# Function to lemmatize and filter POS\n",
    "def lemmatize_and_filter(text, pos_tags):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    words = [word for word in words if word.isalnum() and word.lower() not in stop_words]\n",
    "    pos_words = nltk.pos_tag(words)\n",
    "    filtered_words = [lemmatizer.lemmatize(word.lower()) for word, pos in pos_words if pos in pos_tags]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Apply preprocessing and get the first half of the conversation\n",
    "chat_df['first_half'] = chat_df['full_chat_clean'].apply(preprocess_text).apply(get_first_half)\n",
    "\n",
    "# Create the processed dataset using the first half of the conversations\n",
    "whole_chat_first_half_df = chat_df[['first_half']].copy()\n",
    "whole_chat_first_half_df['processed'] = whole_chat_first_half_df['first_half'].apply(lambda x: lemmatize_and_filter(x, {'NN', 'JJ', 'VB'}))\n",
    "\n",
    "# Function to perform LDA and format topics\n",
    "def perform_lda(text_data, num_topics=13):\n",
    "    vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "    dtm = vectorizer.fit_transform(text_data)\n",
    "    lda = LatentDirichletAllocation(n_components=num_topics, random_state=7)\n",
    "    lda.fit(dtm)\n",
    "    doc_topic_matrix = lda.transform(dtm)\n",
    "    topics = []\n",
    "    for idx, topic in enumerate(lda.components_):\n",
    "        topic_words = [vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-10:]]\n",
    "        topics.append(f\"Topic {idx + 1}.\" + \" \".join(topic_words))\n",
    "    return lda, doc_topic_matrix, topics\n",
    "\n",
    "# Apply LDA on the 'processed' column of the whole_chat_first_half_df\n",
    "num_topics = 13\n",
    "lda_model, doc_topic_matrix, topics = perform_lda(whole_chat_first_half_df['processed'], num_topics)\n",
    "\n",
    "# Assign each document to the most probable topic\n",
    "chat_df['lda_topic'] = doc_topic_matrix.argmax(axis=1)\n",
    "\n",
    "# Convert the 'lda_topic' column to string\n",
    "chat_df['lda_topic'] = chat_df['lda_topic'].astype(str)\n",
    "\n",
    "# Display the topics\n",
    "print(\"LDA Topics:\")\n",
    "for topic in topics:\n",
    "    print(topic)\n",
    "\n",
    "# Display the DataFrame with LDA topics assigned\n",
    "print(\"Chat DataFrame with LDA Topics:\")\n",
    "print(chat_df[['full_chat_clean', 'lda_topic']])\n",
    "\n",
    "# Count the number of records associated with each topic\n",
    "topic_counts = chat_df['lda_topic'].value_counts()\n",
    "print(\"Topic Counts:\")\n",
    "print(topic_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c018db5-aba6-4192-97a9-e59b64ac0293",
   "metadata": {},
   "source": [
    "## pLSA using NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ad301b-5ce2-4e07-9654-0479b9b0237f",
   "metadata": {},
   "source": [
    "The function perform_plsa performs topic modeling on a given set of text data using Probabilistic Latent Semantic Analysis (PLSA) via Non-negative Matrix Factorization (NMF). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c919c0d3-0468-4557-9c50-978245fbad46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/jenn/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/jenn/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/jenn/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to /Users/jenn/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pLSA Topics:\n",
      "Topic 1.create look sure send information address question contact ticket email\n",
      "Topic 2.share additional link question create figure ok record document br\n",
      "Topic 3.number today check return copy borrow pick chapter title book\n",
      "Topic 4.style text caption file sure figure template list table document\n",
      "Topic 5.download look volume click issue title citation link journal article\n",
      "Topic 6.electronic way subscription issue try database able link online access\n",
      "Topic 7.time welcome public information use open contact student card library\n",
      "Topic 8.option need form copy interlibrary scan loan place item request\n",
      "Topic 9.ticket student appreciate information moment today contact able need help\n",
      "Topic 10.specific let click way ok journal database title try search\n",
      "Topic 11.file paragraph break title click number try section link page\n",
      "Topic 12.okay like assist time happy look welcome moment ok thank\n",
      "Topic 13.number renew issue card problem log try check able account\n",
      "Topic 14.course time look moment link let online copy check available\n",
      "Topic 15.moment additional time welcome ask question today sure day great\n",
      "Chat DataFrame with pLSA Topics:\n",
      "                                         full_chat_clean plsa_topic\n",
      "0      I would like to recommend adding to our Pitt r...          7\n",
      "1      sorry our chat disappeared   Hi\\n  : Well here...          7\n",
      "2      I would like to borrow a book from the law lib...          2\n",
      "3      hurricane disaster management   hi\\n  what are...          9\n",
      "4      yes.. looking for an article that comes up at ...          4\n",
      "...                                                  ...        ...\n",
      "10541  Hello! I'm a staff member at Pitt, and I wante...          8\n",
      "10542  I want inquire on the due date of a book I bor...          2\n",
      "10543  I get \"There seems to be an issue with your li...         12\n",
      "10544  what is an ISSN   An ISSN is a number that ide...          4\n",
      "10545  Hi! I was hoping to see if Pitt has access to ...          7\n",
      "\n",
      "[10546 rows x 2 columns]\n",
      "Topic Counts:\n",
      "plsa_topic\n",
      "8     1562\n",
      "2     1034\n",
      "14     833\n",
      "4      782\n",
      "1      751\n",
      "6      731\n",
      "5      724\n",
      "13     697\n",
      "11     621\n",
      "7      609\n",
      "0      553\n",
      "12     545\n",
      "9      511\n",
      "3      378\n",
      "10     215\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Assuming chat_df is already defined with your data\n",
    "# Initialize lemmatizer and stopwords\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "additional_stopwords = set([\n",
    "    \"nameredacted\", \"librarian\", \"hi\", \"hello\"\n",
    "])\n",
    "\n",
    "stop_words = stop_words.union(additional_stopwords)\n",
    "\n",
    "# Function to lemmatize and filter POS\n",
    "def lemmatize_and_filter(text, pos_tags):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    words = [word for word in words if word.isalnum() and word.lower() not in stop_words]\n",
    "    pos_words = nltk.pos_tag(words)\n",
    "    filtered_words = [lemmatizer.lemmatize(word.lower()) for word, pos in pos_words if pos in pos_tags]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Create the processed dataset\n",
    "whole_chat_nouns_adjectives_verbs_df = chat_df[['full_chat_clean']].copy()\n",
    "whole_chat_nouns_adjectives_verbs_df['processed'] = whole_chat_nouns_adjectives_verbs_df['full_chat_clean'].apply(lambda x: lemmatize_and_filter(x, {'NN', 'JJ', 'VB'}))\n",
    "\n",
    "# Function to perform pLSA using NMF\n",
    "def perform_plsa(text_data, num_topics=15):\n",
    "    vectorizer = CountVectorizer(max_df=0.80, min_df=5, stop_words='english')\n",
    "    dtm = vectorizer.fit_transform(text_data)\n",
    "    nmf = NMF(n_components=num_topics, random_state=42)\n",
    "    nmf.fit(dtm)\n",
    "    doc_topic_matrix = nmf.transform(dtm)\n",
    "    topics = []\n",
    "    for idx, topic in enumerate(nmf.components_):\n",
    "        topic_words = [vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-10:]]\n",
    "        topics.append(f\"Topic {idx + 1}.\" + \" \".join(topic_words))\n",
    "    return nmf, doc_topic_matrix, topics\n",
    "\n",
    "# Apply pLSA on the 'processed' column of the whole_chat_nouns_adjectives_verbs_df\n",
    "num_topics = 15\n",
    "nmf_model, doc_topic_matrix, topics = perform_plsa(whole_chat_nouns_adjectives_verbs_df['processed'], num_topics)\n",
    "\n",
    "# Assign each document to the most probable topic\n",
    "chat_df['plsa_topic'] = doc_topic_matrix.argmax(axis=1)\n",
    "\n",
    "# Convert the 'plsa_topic' column to string\n",
    "chat_df['plsa_topic'] = chat_df['plsa_topic'].astype(str)\n",
    "\n",
    "# Display the topics\n",
    "print(\"pLSA Topics:\")\n",
    "for topic in topics:\n",
    "    print(topic)\n",
    "\n",
    "# Display the DataFrame with pLSA topics assigned\n",
    "print(\"Chat DataFrame with pLSA Topics:\")\n",
    "print(chat_df[['full_chat_clean', 'plsa_topic']])\n",
    "\n",
    "# Count the number of records associated with each topic\n",
    "topic_counts = chat_df['plsa_topic'].value_counts()\n",
    "print(\"Topic Counts:\")\n",
    "print(topic_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad9662a-d4f7-433e-ac5c-4d3f0f13d0b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
