{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ded31b18-3413-4523-bf77-186798711fbd",
   "metadata": {},
   "source": [
    "# Chat Topic Extraction Split\n",
    "\n",
    "## Field Experience 2024\n",
    "\n",
    "## Ryan Negron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1b7dfd1-b0cc-45df-a3cd-0c27323993c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6f85840-81a3-4493-b96c-f51bc7073177",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19f9f46a-9baa-4281-b187-46312c1f33b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting fuzzywuzzy\n",
      "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
      "Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
      "Installing collected packages: fuzzywuzzy\n",
      "Successfully installed fuzzywuzzy-0.18.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install fuzzywuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7eb40f71-39a1-4cb8-b148-d4bebf221b38",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting emoji\n",
      "  Downloading emoji-2.12.1-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.7.0 in c:\\tools\\anaconda3\\lib\\site-packages (from emoji) (4.9.0)\n",
      "Downloading emoji-2.12.1-py3-none-any.whl (431 kB)\n",
      "   ---------------------------------------- 0.0/431.4 kB ? eta -:--:--\n",
      "    --------------------------------------- 10.2/431.4 kB ? eta -:--:--\n",
      "   -- ------------------------------------ 30.7/431.4 kB 435.7 kB/s eta 0:00:01\n",
      "   -------- ------------------------------ 92.2/431.4 kB 751.6 kB/s eta 0:00:01\n",
      "   -------------------------- ------------- 286.7/431.4 kB 1.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 431.4/431.4 kB 2.1 MB/s eta 0:00:00\n",
      "Installing collected packages: emoji\n",
      "Successfully installed emoji-2.12.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "481243c8-bf17-412f-bb1b-bce2b3812ad4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chat_df = pd.read_csv('deidentified_transcript.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b1dcf7ce-1638-4fca-bfc6-6adc0e51d666",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from pandas import DataFrame\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from fuzzywuzzy import process\n",
    "import nltk\n",
    "import logging\n",
    "from hashlib import blake2b\n",
    "import re\n",
    "from fuzzywuzzy import fuzz\n",
    "import hashlib\n",
    "import html\n",
    "import emoji\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4ccb3133-7519-4fc4-9bf7-95b21ad734c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10546 entries, 0 to 10545\n",
      "Data columns (total 23 columns):\n",
      " #   Column                         Non-Null Count  Dtype  \n",
      "---  ------                         --------------  -----  \n",
      " 0   Chat ID                        10546 non-null  int64  \n",
      " 1   Browser                        10546 non-null  object \n",
      " 2   Operating System               10541 non-null  object \n",
      " 3   User Agent                     10546 non-null  object \n",
      " 4   Referrer                       10546 non-null  object \n",
      " 5   Widget                         10544 non-null  object \n",
      " 6   Department                     10382 non-null  object \n",
      " 7   Timestamp                      10546 non-null  object \n",
      " 8   Wait Time (seconds)            10546 non-null  int64  \n",
      " 9   Duration (seconds)             10546 non-null  int64  \n",
      " 10  Screensharing                  32 non-null     object \n",
      " 11  Rating (0-4)                   10546 non-null  int64  \n",
      " 12  Comment                        562 non-null    object \n",
      " 13  Transfer History               245 non-null    object \n",
      " 14  Message Count                  10546 non-null  int64  \n",
      " 15  Internal Note                  49 non-null     object \n",
      " 16  Tags                           2 non-null      object \n",
      " 17  Ticket ID                      2111 non-null   float64\n",
      " 18  Unnamed: 27                    116 non-null    object \n",
      " 19  Deidentified_Answerer          10546 non-null  object \n",
      " 20  Deidentified_Email             10546 non-null  object \n",
      " 21  Deidentified_Transcript        10546 non-null  object \n",
      " 22  Deidentified_Initial_Question  9577 non-null   object \n",
      "dtypes: float64(1), int64(5), object(17)\n",
      "memory usage: 1.9+ MB\n"
     ]
    }
   ],
   "source": [
    "chat_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c1b067a5-a0b7-4189-aa0a-46ecda681dc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chat_df['Deidentified_Initial_Question'].fillna('NO INITIAL QUESTION', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8dd26fd2-75e7-4bea-8350-0cf094b2720f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chat_df=chat_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4c27685d-1513-4082-8364-29c00bb524e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chat_df['full_chat'] = chat_df['Deidentified_Initial_Question'] + ' ' + chat_df['Deidentified_Transcript']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e895b564-68bc-4483-a064-66f6407fe5a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I misplaced one of the books I checked out, however, I found the exact copy of the book for purchase online. Can I replace the missing book with this copy. 15:06:22 - Librarian 16 : Hi there\n",
      "15:07:25 - Librarian 16 : Which book did you misplace\n",
      "15:11:39 - Userff8391c23b : A discourse on method\n",
      "15:14:37 - Librarian 16 : Okay thank you for that information\n",
      "15:16:02 - Librarian 16 : It might be possible for you to be able to buy a copy but the librarian in charge of that subject area would need to look into the replacement options for the item\n",
      "15:16:23 - Librarian 16 : so I can create a ticket for them to get back to you and they would contact you next week\n",
      "15:17:19 - Librarian 16 : the item isn't due until 9/9/99\n",
      "15:20:14 - Userff8391c23b : could you create a ticket?\n",
      "15:20:23 - Librarian 16 : yes I will ULSMember that for you\n",
      "15:20:29 - Userff8391c23b : Thank you!\n",
      "15:20:56 - Librarian 16 : Also I suggest double checking that you have truly lost the item in the mean time\n",
      "15:22:19 - Librarian 16 : Is there anything else that I can ULSMember for you today\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(chat_df['full_chat'].iloc[25])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733492bf-4f17-4197-901f-58ff709acd6f",
   "metadata": {},
   "source": [
    "## 1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3d715530-a55c-48c6-8892-d0d2d6db9e57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c6528fc0-533e-404e-9ca6-3f3e99536bce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Remove timestamps, identifiers, URLs, and non-ASCII characters\n",
    "    text = re.sub(r'\\d{2}:\\d{2}:\\d{2}', '', text)\n",
    "    text = re.sub(r'- Librarian \\d+ :', '', text)\n",
    "    text = re.sub(r'User[a-zA-Z0-9]+ :', '', text)\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d08a38eb-7be6-46c2-8dfa-ba672fecbb49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Let's now split text into Librarian And User Chats\n",
    "def extract_conversations_from_transcript(df, column_name):\n",
    "    # Define a regular expression pattern to match \"Timestamp - Librarian X: Utterance\" and \"Timestamp - UserX: Utterance\" format\n",
    "    pattern = re.compile(r'(\\d{2}:\\d{2}:\\d{2}) - (Librarian \\d+ :|User\\w* :) (.*)')\n",
    "    \n",
    "    # Initialize dictionaries to store conversations for Librarian and User\n",
    "    librarian_conversations = {}\n",
    "    user_conversations = {}\n",
    "    \n",
    "    for idx, transcript in enumerate(df[column_name]):\n",
    "        lines = transcript.split('\\n')  # Split transcript into lines\n",
    "        \n",
    "        for line in lines:\n",
    "            match = pattern.match(line)\n",
    "            if match:\n",
    "                timestamp = match.group(1)\n",
    "                speaker = match.group(2)\n",
    "                utterance = match.group(3)\n",
    "                \n",
    "                conversation_index = idx  # Use the index of the original transcript as conversation index so as to not split up the interactions\n",
    "                \n",
    "                if speaker.startswith('Librarian'):\n",
    "                    if conversation_index not in librarian_conversations:\n",
    "                        librarian_conversations[conversation_index] = []\n",
    "                    librarian_conversations[conversation_index].append(utterance.strip())\n",
    "                \n",
    "                elif speaker.startswith('User'):\n",
    "                    if conversation_index not in user_conversations:\n",
    "                        user_conversations[conversation_index] = []\n",
    "                    user_conversations[conversation_index].append(utterance.strip())\n",
    "    \n",
    "    # Create lists for DataFrame creation\n",
    "    data = []\n",
    "    \n",
    "    # Iterate through conversation indexes\n",
    "    for idx in range(len(df)):\n",
    "        librarian_utterance = \" \".join(librarian_conversations.get(idx, ['']))\n",
    "        user_utterance = \" \".join(user_conversations.get(idx, ['']))\n",
    "        \n",
    "        data.append({\n",
    "            'Librarian_Chat': librarian_utterance,\n",
    "            'User_Chat': user_utterance\n",
    "        })\n",
    "    \n",
    "    # Create DataFrame from the list of dictionaries\n",
    "    df_extracted = pd.DataFrame(data)\n",
    "    \n",
    "    return df_extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a39cf1e0-210b-4714-900a-8d95ba9b2cd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "split_chat_df = extract_conversations_from_transcript(chat_df.copy(), 'Deidentified_Transcript')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "027201ef-37e9-428c-83d4-7c6d995e2211",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "split_chat_df['User_Chat_clean'] = split_chat_df['User_Chat'].apply(preprocess_text)\n",
    "split_chat_df['Librarian_Chat_clean'] = split_chat_df['Librarian_Chat'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d18ad0a8-ed13-4542-8945-e43d5eec40a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Appply Preprocess_text function\n",
    "chat_df['full_chat_clean'] = chat_df['full_chat'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "aec40634-78ea-43b3-b6fc-76e8ec02fc1c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Trying to help a faculty member who's in the process of submitting an article for publication with a former graduate student (now alumni). She's asking if there is a way to get that student access to journal articles so that they can finish their response to reviewers. I know the special borrowers card would get her access to print materials, but is there any alternative mechanism for online databases?   [nameredacted] [nameredacted],\\n  at Hillman library the alumni can get a day pass to access our databases in the library\\n  I don't think there is a mechanism that would allow the student to have remote access\\n  maybe a sponsored CSSD account through your department, but I'm not sure if that would provide access to library databases.\\n -  Hmm, OK. If they paid for a special borrowertrade_mark s card, would that have any other benefit that would help them with remotely requesting materials?\\n  the special borrowers card only gives the ability to check out materials from the print collection, so unfortunately no that wouldn't help\\n -  OK. Thanks. Just want to make sure I give them the right information.\\n  sure thing, please ask again if you have any questions.\\n -  Bye.\\n\""
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_df['full_chat_clean'].iloc[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7870517b-eac8-42a4-ac16-13851a375cf9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hmm, OK. If they paid for a special borrowertrade_mark s card, would that have any other benefit that would help them with remotely requesting materials? OK. Thanks. Just want to make sure I give them the right information. Bye.'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_chat_df['User_Chat_clean'].iloc[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cd60006f-0158-450b-b6a8-66a67f4ff9cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[nameredacted] [nameredacted], at Hillman library the alumni can get a day pass to access our databases in the library I don't think there is a mechanism that would allow the student to have remote access maybe a sponsored CSSD account through your department, but I'm not sure if that would provide access to library databases. the special borrowers card only gives the ability to check out materials from the print collection, so unfortunately no that wouldn't help sure thing, please ask again if you have any questions.\""
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_chat_df['Librarian_Chat_clean'].iloc[6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fdaa5e-9322-4ca0-a1d9-6d9415897ffe",
   "metadata": {},
   "source": [
    "## TEST 1. NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ccb6d8f5-1fdc-4a1b-aaeb-59f9fba06fde",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\rnn13\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\rnn13\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\rnn13\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rnn13\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rnn13\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "# Initialize lemmatizer and stopwords\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfadbd8c-ab6a-4e35-a961-38a52d18bab9",
   "metadata": {},
   "source": [
    "### Lemmatizes and Filters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3320ab8b-c383-4fa5-aec0-196047f2bc65",
   "metadata": {},
   "source": [
    "The text based on specified parts of speech (POS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5e75b841-a605-4738-a9ac-668e23c67676",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "additional_stopwords = set([\n",
    "    \"nameredacted\", \"librarian\", \"hi\", \"hello\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b6a08326-3b33-4edf-92f8-92fa16e5164a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')).union(additional_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2aa0925f-a468-4802-b999-8f429f9c5481",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to lemmatize and filter POS\n",
    "def lemmatize_and_filter(text, pos_tags):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    words = [word for word in words if word.isalnum() and word.lower() not in stop_words]\n",
    "    pos_words = nltk.pos_tag(words)\n",
    "    filtered_words = [lemmatizer.lemmatize(word.lower()) for word, pos in pos_words if pos in pos_tags]\n",
    "    return ' '.join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "bce14b7c-55e7-4b37-8aa8-c90a70f944e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the different datasets\n",
    "user_only_df = split_chat_df[['User_Chat_clean']].copy()\n",
    "user_only_df['processed'] = user_only_df['User_Chat_clean'].apply(lambda x: lemmatize_and_filter(x, {'NN', 'JJ', 'VB'}))\n",
    "\n",
    "lib_only_df = split_chat_df[['Librarian_Chat_clean']].copy()\n",
    "lib_only_df['processed'] = lib_only_df['Librarian_Chat_clean'].apply(lambda x: lemmatize_and_filter(x, {'NN', 'JJ', 'VB'}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "95781a5e-9a6f-4b27-b7b1-7e91ce013366",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nLet's try a TfidfVectorizer instead like with the other model. This is initialized to convert the text data into a document-term matrix (DTM). The parameters used are:\\n\\t•\\tmax_df=0.95: Ignore terms that appear in more than 80% of the documents.\\n\\t•\\tmin_df=2: Ignore terms that appear in fewer than 5 documents.\\n\\t•\\tstop_words='english': Remove common English stop words.\\n\""
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to perform LDA and format topics\n",
    "def perform_lda(text_data, num_topics=15):\n",
    "    vectorizer = TfidfVectorizer(max_df=0.80, min_df=5, stop_words='english')\n",
    "    dtm = vectorizer.fit_transform(text_data)\n",
    "    lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "    lda.fit(dtm)\n",
    "    topics = []\n",
    "    for idx, topic in enumerate(lda.components_):\n",
    "        topic_words = [vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-10:]]\n",
    "        topics.append(f\"Topic {idx + 1}.\" + \" \".join(topic_words))\n",
    "    return topics\n",
    "\n",
    "\"\"\"\n",
    "Let's try a TfidfVectorizer instead like with the other model. This is initialized to convert the text data into a document-term matrix (DTM). The parameters used are:\n",
    "\t•\tmax_df=0.95: Ignore terms that appear in more than 80% of the documents.\n",
    "\t•\tmin_df=2: Ignore terms that appear in fewer than 5 documents.\n",
    "\t•\tstop_words='english': Remove common English stop words.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9974b9e6-3b32-43b4-a4b0-4fd06116e89b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply LDA on the different datasets\n",
    "user_only_topics = perform_lda(user_only_df['processed'])\n",
    "lib_only_topics = perform_lda(lib_only_df['processed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1cc3065f-548d-42e4-bffe-1aa3d9558206",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Topic 1.pp scan great computer open help thank floor idea sure',\n",
       " 'Topic 2.lot work helpful sure think problem help thank day great',\n",
       " 'Topic 3.come help ok thank tomorrow today return time library book',\n",
       " 'Topic 4.course trouble oh help try sure edition okay ok thank',\n",
       " 'Topic 5.great know response ticket email awesome thank contact day nice',\n",
       " 'Topic 6.page table book thing dissertation email pick address thank sure',\n",
       " 'Topic 7.sense great emailredacted ok help student thank sorry email access',\n",
       " 'Topic 8.different research article book place great thank interlibrary request loan',\n",
       " 'Topic 9.help possible day information phone morning number copy look wonderful',\n",
       " 'Topic 10.way great help library thank card class week book check',\n",
       " 'Topic 11.click try help ok journal thank page access link article',\n",
       " 'Topic 12.day renew great reach assistance fine thank perfect appreciate help',\n",
       " 'Topic 13.print electronic thank online book file version question afternoon thx',\n",
       " 'Topic 14.email day bye correct open ok try message thank nope',\n",
       " 'Topic 15.check book style help member library thank campus faculty hillman']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_only_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "682546c1-039a-48c1-a1a2-787f17710e28",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Topic 1.moment day touch address contact create help question email ticket',\n",
       " 'Topic 2.emailredacted thesis test mailto law school help permission dissertation copyright',\n",
       " 'Topic 3.info send moment problem email let help term pas message',\n",
       " 'Topic 4.title database try article click page link access search br',\n",
       " 'Topic 5.great day expert ticket healthy book look thing sure request',\n",
       " 'Topic 6.edition reserve pick library item copy available check request book',\n",
       " 'Topic 7.check moment link interlibrary loan let journal request access article',\n",
       " 'Topic 8.content available time like welcome assist look moment happy thank',\n",
       " 'Topic 9.current help study library member equipment room staff student faculty',\n",
       " 'Topic 10.help assist email free feel day great inquiry additional br',\n",
       " 'Topic 11.table page style share file template school document br help',\n",
       " 'Topic 12.contact book date moment check renew number card library account',\n",
       " 'Topic 13.initiate discipline store troubleshoot live invitation change lend communication li',\n",
       " 'Topic 14.linkredacted specific moment help question contact information research citation guide',\n",
       " 'Topic 15.entrance desk welcome lab afternoon return open drop ground floor']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lib_only_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c018db5-aba6-4192-97a9-e59b64ac0293",
   "metadata": {},
   "source": [
    "## pLSA for Split Chat data using NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ad301b-5ce2-4e07-9654-0479b9b0237f",
   "metadata": {},
   "source": [
    "The function perform_plsa performs topic modeling on a given set of text data using Probabilistic Latent Semantic Analysis (PLSA) via Non-negative Matrix Factorization (NMF). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c919c0d3-0468-4557-9c50-978245fbad46",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\rnn13\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\rnn13\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\rnn13\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rnn13\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user pLSA Topics:\n",
      "Topic 1.fine make wait use research renew version title yes help\n",
      "Topic 2.site login database campus subscription account way online journal access\n",
      "Topic 3.like title pick library edition borrow return copy chapter book\n",
      "Topic 4.message care fine number response ticket wait send address email\n",
      "Topic 5.log sign copy search issue message work let campus try\n",
      "Topic 6.volume way citation download issue loan search title journal article\n",
      "Topic 7.understand weekend ask perfect course sense hope rest assistance appreciate\n",
      "Topic 8.lot weekend care year week person thing use course time\n",
      "Topic 9.week pick submit option place copy item loan way request\n",
      "Topic 10.online class pick tomorrow like status week account let check\n",
      "Topic 11.year message file class site text click send page link\n",
      "Topic 12.fine number copy address yes message assistance account issue problem\n",
      "Topic 13.author place edition thing search sense dont database look think\n",
      "Topic 14.look number know page question information contact student work need\n",
      "Topic 15.card week library perfect weekend class pick tomorrow campus today\n",
      "librarian pLSA Topics:\n",
      "Topic 1.make work file course time try copy look know let\n",
      "Topic 2.minute share caption school file record hesitate ask document br\n",
      "Topic 3.response chat stay course follow make thing expert create ticket\n",
      "Topic 4.course borrow return drop reserve number chapter title copy book\n",
      "Topic 5.problem member share template document need student school today help\n",
      "Topic 6.review inquiry like reach today item hesitate time look moment\n",
      "Topic 7.need submit option service form copy place item loan request\n",
      "Topic 8.team faculty print campus online issue problem journal database access\n",
      "Topic 9.eye department message area problem send pas reach address email\n",
      "Topic 10.campus reserve service edition floor let use moment catalog check\n",
      "Topic 11.catalog box work database click title try page link search\n",
      "Topic 12.specialist service department today refer best review answer read question\n",
      "Topic 13.file link subscription volume loan issue title citation journal article\n",
      "Topic 14.item library log today renew problem date number card account\n",
      "Topic 15.assist need hesitate floor time department service info information contact\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "\n",
    "# Assuming chat_df is already defined with your data\n",
    "# Initialize lemmatizer and stopwords\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "additional_stopwords = set([\n",
    "    \"nameredacted\", \"librarian\", \"hi\", \"hello\", \"thank\", \"sorry\", \"nope\", \"ok\", \"okay\", \"oh\", \"morning\", \"welcome\", \"afternoon\", \"day\", \"nice\", \"happy\", \"wonderful\", \n",
    "])\n",
    "\n",
    "stop_words = stop_words.union(additional_stopwords)\n",
    "\n",
    "# Function to lemmatize and filter POS\n",
    "def lemmatize_and_filter(text, pos_tags):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    words = [word for word in words if word.isalnum() and word.lower() not in stop_words]\n",
    "    pos_words = nltk.pos_tag(words)\n",
    "    filtered_words = [lemmatizer.lemmatize(word.lower()) for word, pos in pos_words if pos in pos_tags]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Create the processed dataset\n",
    "user_plsa_df = split_chat_df[['User_Chat_clean']].copy()\n",
    "user_plsa_df['processed'] = user_plsa_df['User_Chat_clean'].apply(lambda x: lemmatize_and_filter(x, {'NN','VB'}))\n",
    "lib_plsa_df = split_chat_df[['Librarian_Chat_clean']].copy()\n",
    "lib_plsa_df['processed'] = lib_plsa_df['Librarian_Chat_clean'].apply(lambda x: lemmatize_and_filter(x, {'NN', 'VB'}))\n",
    "\n",
    "\n",
    "\n",
    "# Function to perform pLSA using NMF\n",
    "def perform_plsa(text_data, num_topics=15):\n",
    "    vectorizer = TfidfVectorizer(max_df=0.80, min_df=5, stop_words='english')\n",
    "    dtm = vectorizer.fit_transform(text_data)\n",
    "    nmf = NMF(n_components=num_topics, random_state=42)\n",
    "    nmf.fit(dtm)\n",
    "    doc_topic_matrix = nmf.transform(dtm)\n",
    "    topics = []\n",
    "    for idx, topic in enumerate(nmf.components_):\n",
    "        topic_words = [vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-10:]]\n",
    "        topics.append(f\"Topic {idx + 1}.\" + \" \".join(topic_words))\n",
    "    return nmf, doc_topic_matrix, topics\n",
    "\n",
    "# Apply pLSA on the 'processed' column of both user and lib dataframes\n",
    "num_topics = 15\n",
    "user_nmf_model, user_doc_topic_matrix, user_topics = perform_plsa(user_plsa_df['processed'], num_topics)\n",
    "lib_nmf_model, lib_doc_topic_matrix, lib_topics = perform_plsa(lib_plsa_df['processed'], num_topics)\n",
    "\n",
    "\n",
    "# Assign each document to the most probable topic for both lib and user\n",
    "split_chat_df['user_plsa_topic'] = user_doc_topic_matrix.argmax(axis=1)\n",
    "split_chat_df['lib_plsa_topic'] = lib_doc_topic_matrix.argmax(axis=1)\n",
    "\n",
    "\n",
    "# Convert the 'plsa_topic' column to string\n",
    "split_chat_df['user_plsa_topic'] = split_chat_df['user_plsa_topic'].astype(str)\n",
    "split_chat_df['lib_plsa_topic'] = split_chat_df['lib_plsa_topic'].astype(str)\n",
    "\n",
    "# Display the topics\n",
    "print(\"user pLSA Topics:\")\n",
    "for topic in user_topics:\n",
    "    print(topic)\n",
    "    \n",
    "    \n",
    "    \n",
    "print(\"librarian pLSA Topics:\")\n",
    "for topic in lib_topics:\n",
    "    print(topic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "9ad9662a-d4f7-433e-ac5c-4d3f0f13d0b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Librarian_Chat          Hi there Which book did you misplace Okay than...\n",
       "User_Chat               A discourse on method could you create a ticke...\n",
       "User_Chat_clean         A discourse on method could you create a ticke...\n",
       "Librarian_Chat_clean    Hi there Which book did you misplace Okay than...\n",
       "user_plsa_topic                                                        13\n",
       "lib_plsa_topic                                                          3\n",
       "Name: 25, dtype: object"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_chat_df.iloc[25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99493466-c4ee-4861-aaf5-6754ea40476e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
